# Docker Image Optimization: Shrinking Images by Up to 99%

## The Hidden Cost of Bloated Docker Images

Oversized Docker images aren't just a nuisance â€” they're costing you:

- **Time**: Slower build and deployment cycles
- **Money**: Increased storage and bandwidth costs
- **Performance**: Reduced application responsiveness

## Case Study: 1.2GB to 8MB (99.33% reduction)

The following techniques were applied to a Python-based machine learning application:
- Multi-stage builds
- Layer optimizations
- Minimal base images (including Scratch)
- Advanced techniques like distroless images
- Security best practices

## Starting Point: The Bloated Image

A typical Dockerfile that leads to bloated images:

```dockerfile
FROM python:3.9
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "main.py"]
```

This results in a large image due to:
- Using a full Python image
- Including unnecessary build tools and dependencies
- Inefficient layer caching
- Potential inclusion of unnecessary files

## Optimization Techniques

### 1. Multi-Stage Builds: The Game-Changer

Multi-stage builds separate build-time dependencies from runtime ones.

**Single-Stage Dockerfile (1.2GB)**:
```dockerfile
# Use an official Python runtime as a parent image
FROM python:3.9-slim
# Install necessary build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    gcc \
    && rm -rf /var/lib/apt/lists/*
# Set the working directory
WORKDIR /app
# Copy the requirements file and install dependencies
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
# Copy the rest of the application code
COPY . .
# Compile the model (if necessary)
RUN python compile_model.py
# Run the inference script
CMD ["python", "inference.py"]
```

**Multi-Stage Dockerfile (85MB)**:

Stage 1: Build Stage
```dockerfile
# Stage 1: Build
FROM python:3.9-slim AS builder
# Install necessary build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    gcc \
    && rm -rf /var/lib/apt/lists/*
# Set the working directory
WORKDIR /app
# Copy the requirements file and install dependencies
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
# Copy the application code
COPY . .
# Compile the model (if necessary)
RUN python compile_model.py
# Install PyInstaller
RUN pip install pyinstaller
# Create a standalone executable
RUN pyinstaller --onefile inference.py
```

Stage 2: Production Stage
```dockerfile
# Stage 2: Production
FROM scratch
# Set the working directory
WORKDIR /app
# Copy only the necessary files from the build stage
COPY --from=builder /app/dist/inference /app/inference
COPY --from=builder /app/model /app/model
# Run the inference executable
ENTRYPOINT ["/app/inference"]
```

### 2. Layer Optimization: Every Byte Counts

Each instruction in your Dockerfile creates a new layer. Minimize layers by combining commands:

```dockerfile
RUN apt-get update && apt-get install -y python3-pip python3-dev && \
    pip3 install numpy pandas && \
    apt-get clean && rm -rf /var/lib/apt/lists/*
```

### 3. Minimal Base Images from Scratch

The "scratch" base image has no operating system, no dependencies - just an empty storage container.

Useful for:
- Creating your own base image
- Deploying standalone executable applications

```dockerfile
# syntax=docker/dockerfile:1
FROM scratch
ADD myapp /
CMD ["/myapp"]
```

### 4. Advanced Techniques: Distroless Images

Google's distroless images provide a middle ground between full distributions and scratch images:
- Smaller than full distribution images
- More secure with fewer unnecessary components
- Include important elements like SSL certificates and timezone data

```dockerfile
FROM gcr.io/distroless/python3-debian10
COPY --from=builder /app/dist/main /app/main
COPY --from=builder /app/model /app/model
COPY --from=builder /app/config.yml /app/config.yml
ENTRYPOINT ["/app/main"]
```

### 5. Other Optimization Techniques

- **Use Docker BuildKit**: Enables improved performance and security
  ```
  DOCKER_BUILDKIT=1 docker build -t myapp .
  ```

- **Eliminate Unnecessary Files**: Store data externally in volumes or databases instead of within the image

- **Use .dockerignore**: Exclude specific files and directories from your final image
  ```
  # Exclude large datasets
  data/
  # Exclude virtual environment
  venv/
  # Exclude cache, logs, and temporary files
  __pycache__/
  *.log
  *.tmp
  *.pyc
  *.pyo
  *.pyd
  .pytest_cache
  .git
  .gitignore
  README.md
  # Exclude model training checkpoints and tensorboard logs
  checkpoints/
  runs/
  ```

- **Leverage Image Analysis Tools**: Use tools like Dive and Docker Slim to analyze and identify redundant layers

- **Unikernels**: Create specialized, minimal images (can be 80% smaller than typical Docker images)

## Security Best Practices

1. **Use Trusted and Official Base Images**: Avoid unverified images from unknown sources

2. **Run Containers as Non-Root Users**:
   ```dockerfile
   RUN adduser --disabled-password --gecos "" appuser
   USER appuser
   ```

3. **Limit Network Exposure**:
   ```
   docker run -p 127.0.0.1:8080:8080 myimage
   ```

4. **Scan Images for Vulnerabilities**:
   ```
   docker scan your-image:tag
   ```

5. **Avoid Hardcoding Sensitive Information**: Use Docker secrets or environment variables managed by orchestration tools

6. **Enable Logging and Monitoring**: Track suspicious activities

## Results

After implementing these techniques:
- Image Size: Reduced from 1.2GB to 8MB (99.33% reduction)
- Deployment Time: Cut by 85%
- Cloud Costs: Reduced by 60%

## Starting Points

1. Begin with multi-stage builds
2. Use minimal base images
3. Optimize layers and dependencies
4. Implement security best practices

By applying these optimization techniques, you can achieve significant reductions in your Docker image sizes, improving deployment speed and reducing costs.
